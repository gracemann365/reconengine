## In-Depth Analysis of the Orchestrator Module

### 1. High-Level Purpose

The **Orchestrator** module is a specialized microservice within the larger "Reconciliation Engine." Its primary responsibility is to act as a central traffic controller for incoming financial transaction data.

Its core purpose is to:
1.  **Consume** individual transaction records (called `Quant` objects) from a unified input stream.
2.  **Co-locate and Pair** these records based on a shared `transactionId`. The assumption is that for every transaction, two records will eventually arrive (e.g., one from a payment processor like Visa, and one from an internal ledger).
3.  **Route** successfully formed pairs downstream for detailed, attribute-level matching.
4.  **Isolate and Escalate** records that fail to find their partner within a specific time window, flagging them as incomplete.

It is designed to be highly concurrent and scalable, processing a high volume of transactions in parallel.

---

### 2. The End-to-End Data Flow

Here is the step-by-step journey of a `Quant` object through the module:

#### Step 1: Ingestion - The Front Door

*   **File:** `UnifiedQuantConsumer.java`
*   **Mechanism:** The process begins when a message arrives on a Kafka topic named **`UnifiedDTOs_Input`**. The `@KafkaListener` annotation in this class makes it the entry point for all data.
*   **Action:** The `listen` method is triggered for each message. It deserializes the message value into a `Quant` object.
*   **Buffering:** Instead of processing the `Quant` immediately, it places the object into a thread-safe queue called `ingressBuffer`. This is a crucial design choice that decouples the rate of data consumption from the rate of processing, preventing backpressure on the Kafka consumer itself. It has a capacity limit (`MAX_BUFFER_CAPACITY`) to avoid running out of memory, and will log a warning if it has to drop data.

#### Step 2: Routing and Distribution

*   **File:** `BatchPreparationService.java`
*   **Mechanism:** This service is the heart of the orchestration logic. When the application starts, the `@PostConstruct` `init()` method kicks off a `ScheduledExecutorService`.
*   **Action:** This service runs a task (`pollAndRouteQuants`) every 10 milliseconds. This task pulls one `Quant` object from the `ingressBuffer` (where the consumer placed it).
*   **Sharding Logic:** To process transactions concurrently while ensuring related records are handled together, the service uses a sharding (or partitioning) technique. It calculates the hash of the `transactionId` and uses the modulo operator (`% NUM_PROCESSING_UNITS`) to assign the `Quant` to one of **8** dedicated `ProcessingUnit` instances. This guarantees that two `Quant` objects with the same `transactionId` will always be sent to the exact same `ProcessingUnit`.

#### Step 3: The Core Logic - Pairing and Timeouts

*   **File:** `BatchPreparationService.java` (specifically, the inner class `ProcessingUnit`)
*   **Mechanism:** Each of the 8 `ProcessingUnit` instances operates independently, managing its own set of transactions. This is where the main stateful work happens.
*   **The "Pairing Arena":** Each unit has a `Map<String, TimedQuant>` called `pairingArena`. This is a waiting area where a `Quant` waits for its partner to arrive. The map key is the `transactionId`. The `Quant` is wrapped in a `TimedQuant` object to record its arrival timestamp.

*   **The Pairing Process (`processQuant` method):**
    1.  When a `Quant` arrives, the `ProcessingUnit` checks the `pairingArena` for an existing entry with the same `transactionId`.
    2.  **If a match is found:**
        *   A new `QuantPair` object is created, containing both the existing `Quant` and the newly arrived one.
        *   The original `Quant` is removed from the `pairingArena`.
        *   The newly created `QuantPair` is placed into another queue, `pairedQuantsBuffer`, ready for batching and downstream sending.
    3.  **If no match is found:**
        *   The `Quant` is wrapped in a `TimedQuant` and placed into the `pairingArena` to wait for its partner.

*   **The Timeout Process (`run` method):**
    *   The `ProcessingUnit` is also a `Runnable` and is scheduled to execute periodically.
    *   During its run, it scans the `pairingArena`. It checks the entry time of each waiting `TimedQuant`.
    *   If a `Quant` has been waiting for more than 30 seconds (`INCOMPLETE_PAIR_TIMEOUT_MS`), it is considered an orphan. It is removed from the arena and escalated.

#### Step 4: Egress - The Exit Doors

The module has two distinct exit paths, both using Kafka producers.

**A) The "Happy Path": Matched Pairs**

*   **Files:** `BatchPreparationService.java` (the `flushPairedQuants` method) and `MatchingProducerService.java`.
*   **Mechanism:** The `ProcessingUnit` periodically checks its `pairedQuantsBuffer`. A flush is triggered if either the number of pairs in the buffer reaches 100 (`BATCH_SIZE_THRESHOLD`) or if 5 seconds have passed since the last flush (`FLUSH_INTERVAL_MS`).
*   **Action:**
    1.  A batch of `QuantPair` objects is drained from the buffer into a `List`.
    2.  This list is passed to the `MatchingProducerService`.
    3.  This service sends the entire `List<QuantPair>` as a single message to the **`Matching_Input_Topic`** on Kafka. This batch is sent with the `transactionId` of the first pair as the Kafka message key, which helps the downstream consumer process related batches in order.

**B) The "Exception Path": Incomplete Quants**

*   **Files:** `BatchPreparationService.java` (the `run` method) and `EscalationProducerService.java`.
*   **Mechanism:** This path is triggered by the timeout mechanism described in Step 3.
*   **Action:**
    1.  When a `Quant` is identified as timed-out, a new `IncompleteQuant` object is created, containing the original `Quant` data, the time of escalation, and the reason ("Pairing timeout").
    2.  This object is passed to the `EscalationProducerService`.
    3.  This service sends the `IncompleteQuant` object as a message to the **`Escalation_Topic`** on Kafka. This allows a separate system or process to handle these problematic transactions (e.g., for manual review, logging, or alerting).

---

### 4. Configuration and Setup

*   **`pom.xml`:** Defines the project's dependencies, primarily Spring Boot starters (for web, actuator), Spring for Apache Kafka, and Lombok (to reduce boilerplate code like getters/setters).
*   **`application.yml`:** Contains the runtime configuration for the microservice. This includes the Kafka bootstrap server address, consumer group ID, and various performance-tuning properties for the Kafka consumer and producers (e.g., `retries`, `batch-size`, `linger-ms`). It also sets the application's server port to `8081`.
*   **`OrchestratorConfig.java`:** Defines the `ScheduledExecutorService` as a shared Spring `@Bean`, which is then injected into the `BatchPreparationService` to run its polling and timeout tasks.

In summary, the `orchestrator` is a sophisticated, stateful routing and processing service. It acts as the crucial middle layer in a reconciliation pipeline, intelligently sorting high-volume transactional data into paired batches for further processing or escalating individual records that fail to match.
