// ===================================================================
// Reconciliation Definition Language (RDL) v1.4
// Single Source of Truth for the Reconciliation Engine PoC
// ===================================================================

// -------------------------------------------------------------------
// 1. Global Project & Architectural Definitions
// -------------------------------------------------------------------

project_scope {
  name                   = "Reconciliation Engine (PoC)"
  classification         = "mission_critical_banking_system_poc_non_prod"
  criticality_level      = "tier_1_poc"
  primary_constraint     = "30_minute_sla"
  volume_constraint      = "10000_transactions"
  processing_model       = "eod_batch_one_shot"
  architecture_principle = "sun_do_more_with_less"
  poc_showcase_requirement = "demonstrate_prod_skills_in_non_prod_env"
  junior_engineer_feasibility = "high_automation_clear_design_manageable_learning_curve_efficient_delivery"
  quant_message_format   = "udp_packet_like_self_contained_kafka_transport"
}

architectural_contracts {
  id                      = "RECONCILIATION-ENGINE-CORE-V1.0-20250704-OMNI"
  architecture_type       = "microservices"
  transport_backbone      = "Apache Kafka"
  semantic_principles {
    microservice_decomposition = "independently_deployable_units"
    qa_test_harness_inclusion = "mandatory_for_poc_showcase"
  }
  mandated_libraries_overview = [
    "central_common_libraries (shared DTOs, validation, config, security, fault-tolerance, error-handling, logging/monitoring)"
  ]
}

// -------------------------------------------------------------------
// 2. Shared Resources (Topics, Databases, Libraries)
// -------------------------------------------------------------------

topic "UnifiedDTOs_Input" {}
topic "Matching_Input_Topic" {}
topic "Matching_Output_Topic" {}
topic "Escalation_Topic" {}
topic "Recon_Metadata_Topic" {}
topic "Maas_DLQ_Topic" {}
topic "ure_dlq_topic" {}

database "ure_mongo_db" {
  type = "MongoDB"
}

library "common" {
  id = "CENTRAL-LIB-UNIFIED-V1.0-20250706-POC"
  scope = "PoC"
  cir = 98
  adi = 100
  purpose = "Unified central utility library for reconciliation microservices"
  deployment_type = "Shaded JAR"

  package "dto" {
    purpose = "Unified DTO schema, Avro encoding, error enums"
    behavior = "Defines and version-controls immutable Avro DTOs shared across microservices"
    construction_pattern = "Builder pattern, standard error enums"
    file "Quant.java" {
      purpose = "Represents a single, unified financial transaction."
    }
    file "URE.java" {
      purpose = "Data Transfer Object for Unreconciled Exceptions."
    }
    file "ReasonCode.java" {
      purpose = "Enum for categorizing reasons for UREs."
    }
    file "UreStatus.java" {
      purpose = "Enum for tracking the status of a URE in the workflow."
    }
    file "ReconMetadata.java" {
      purpose = "Stores aggregated metadata and statistics for a reconciliation job or batch."
    }
    file "ErrorCode.java" {
      purpose = "An enumeration of standardized error codes across the entire system."
    }
    schema "Quant.avsc" {}
    schema "URE.avsc" {}
    schema "ReconMetadata.avsc" {}
  }

  package "validation" {
    purpose = "Custom validators, YAML rule engine"
    behavior = "Executes YAML-driven rule evaluations with layered exception hierarchy"
    performance_p99_latency_us = 100
    file "YamlRuleEngine.java" {
      purpose = "Core component that interprets and executes dynamic validation rules."
    }
    file "Validator.java" {
      purpose = "Generic interface for defining custom data validation logic."
    }
    file "ValidationException.java" {
      purpose = "Custom exception specifically for validation failures."
    }
    file "RuleResult.java" {
      purpose = "Encapsulates the outcome of a single validation rule execution."
    }
    resource "rules.yml" {
        purpose = "Example YAML schema for validation rules."
    }
  }

  package "config" {
      purpose = "Dynamic config loaders, feature flags, secrets"
      behavior = "Loads config from env, system props, and YAML; supports hot reload"
      secret_management = "Fetches and rotates secrets via Vault/AWS Secrets Manager"
      file "AppConfig.java" {
          purpose = "Centralized access point for application-wide configurations."
      }
      file "FeatureFlagService.java" {
          purpose = "Manages and provides runtime access to feature toggles."
      }
      file "SecretManager.java" {
          purpose = "Component for securely retrieving and managing sensitive credentials."
      }
  }

  package "pci" {
      purpose = "PAN masking, PCI audit hooks"
      behavior = "Masks PANs and generates PCI audit events; Luhn and expiry validation included"
      security_constraint = "No in-memory retention of sensitive cardholder data"
      file "PanMasker.java" {
          purpose = "Utility to mask sensitive Primary Account Numbers (PANs)."
      }
      file "PciAuditService.java" {
          purpose = "Publishes standardized audit events specifically related to PCI-sensitive data access or processing."
      }
      file "LuhnValidator.java" {
          purpose = "Implements the Luhn algorithm for basic credit card number validation."
      }
      file "ExpiryValidator.java" {
          purpose = "Validates credit card expiry dates."
      }
  }

  package "security" {
      purpose = "TLS setup, JWT parsing, sanitization"
      behavior = "Handles TLS, JWT, OAuth2 token parsing, and input sanitization"
      encryption_helpers = "AES-GCM encryption"
      file "JwtUtil.java" {
          purpose = "Utility for handling JSON Web Tokens (parsing, validation, potential generation)."
      }
      file "InputSanitizer.java" {
          purpose = "Cleans and sanitizes user or external inputs to prevent common injection attacks."
      }
      file "TlsConfig.java" {
          purpose = "Helper for configuring TLS/SSL in client/server contexts."
      }
      file "AesGcmEncryptor.java" {
          purpose = "Provides AES-GCM encryption and decryption functionalities."
      }
  }

  package "resilience" {
      purpose = "Circuit breakers, retries, timeouts"
      behavior = "Applies retries, circuit breakers, timeouts, and fallback patterns"
      integration = "Spring annotations + internal registries"
      file "CircuitBreakerConfig.java" {
          purpose = "Defines configuration settings for Resilience4j Circuit Breaker patterns."
      }
      file "RetryConfig.java" {
          purpose = "Defines configuration settings for Resilience4j Retry mechanisms."
      }
      file "TimeoutConfig.java" {
          purpose = "Defines configuration settings for Resilience4j Timeout patterns."
      }
      file "ResilienceAspects.java" {
          purpose = "Spring AOP aspect that applies resilience patterns via annotations."
      }
  }

  package "error" {
      purpose = "Custom exceptions, DLQ routing, trace injection"
      behavior = "Propagates typed exceptions and maps to HTTP status codes"
      trace_id_injection = "MDC + DLQ routing logic"
      file "ReconciliationException.java" {
          purpose = "Base custom exception for all reconciliation-specific business errors."
      }
      file "DlqRouter.java" {
          purpose = "Service responsible for routing failed messages or data to Dead Letter Queues (DLQs)."
      }
      file "ErrorCategory.java" {
          purpose = "Enumeration for classifying different types of errors."
      }
      file "TraceIdInjector.java" {
          purpose = "Utility to inject and manage distributed trace IDs."
      }
  }

  package "monitoring" {
      purpose = "Structured logs, metrics, audit spans"
      behavior = "Emits structured logs, custom metrics, and OpenTelemetry traces"
      audit_logging = "PCI events routed to audit logger stream"
      file "StructuredLogger.java" {
          purpose = "Utility for emitting machine-readable structured log events."
      }
      file "MetricService.java" {
          purpose = "Central service for publishing application-specific custom metrics to Micrometer."
      }
      file "TraceService.java" {
          purpose = "Integrates with OpenTelemetry for publishing distributed tracing spans."
      }
      file "AuditLogger.java" {
          purpose = "Dedicated logger for capturing critical audit events."
      }
  }

  overall_performance {
    p99_call_latency_ms = 1
    code_coverage_percent = 90
  }

  external_dependencies = [
    "SLF4J",
    "Micrometer",
    "Avro",
    "OpenTelemetry",
    "Resilience4j"
  ]
}

// -------------------------------------------------------------------
// 3. Microservice Definitions
// -------------------------------------------------------------------

service "naas" {
  id = "NAAS-POC-V1.0-20250706"
  scope = "PoC"
  cir = 98
  adi = 100
  sla_slice = "2min"

  produces_to = topic.UnifiedDTOs_Input

  uses = [
    common.dto,
    common.validation,
    common.config,
    common.pci,
    common.security,
    common.resilience,
    common.error,
    common.monitoring
  ]

  stage "file_ingestion" {
    sources = ["Switch SQL Dumps", "VISA CSV Files"]
    mechanism = "SFTP/FTPS with SHA256 checksum validation"
    volume_max_tx_combined_per_day = 10000
    trigger = "Webhook API (OAuth2, idempotent)"
    arrival_window = "23:00â€“23:15 IST"
  }

  stage "etl" {
    parsing_mechanism = "Stream-based: Apache CSV, custom SQL parser"
    concurrency_thread_pool_size = 8
    output_format = "Source-specific JSON DTOs"
    error_handling_dlq = "naas-etl-dlq"
  }

  stage "validation" {
    rules_configuration = "YAML-configured via common_config_utils"
    concurrency_thread_pool_size = 8
    rule_sets = ["Switch domain-specific", "VISA domain-specific"]
    dlq = "naas-validation-dlq"
  }

  stage "queueing" {
    mechanism = "In-memory ConcurrentLinkedQueue"
    capacity_dtos = 5000
    behavior = "Backpressure on overflow"
  }

  stage "normalization" {
    mapping_mechanism = "YAML-based, in-memory lookup"
    concurrency_thread_pool_size = 8
    output_format = "UnifiedDTO (Quants)"
    dlq = "naas-normalization-dlq"
  }

  stage "kafka_publishing" {
    topic = topic.UnifiedDTOs_Input
    format = "Avro, Schema Registry"
    partition_key = "transactionId"
    adaptive_pacing = true
    dlq_behavior = "Halt and alert on persistent failures"
  }

  stage "monitoring_observability" {
    metrics = ["ETL throughput", "Validation throughput", "Queue depth", "DLQ count"]
    alerts = "Failure rates > thresholds trigger MonaaS"
    tracing = "OpenTelemetry from Ingestion to Kafka"
  }

  cross_cutting_concerns {
    fault_tolerance_application {
      retries = "Configured for external calls (e.g., SFTP, Webhook)"
      circuit_breakers = "Applied to external dependencies (e.g., Webhook, Kafka Producer)"
      timeouts = "Configured for all external I/O operations"
    }
    testing_strategy {
      unit_tests = "High coverage for business logic (ETL, Validation, Normalization)"
      integration_tests = "Kafka producer/consumer interactions, file ingestion"
      performance_tests = "Simulated file ingestion up to 10,000 tx/day to validate SLA slice"
      contract_tests = "Schema compatibility with UnifiedDTOs_Input topic"
    }
  }
}

service "orchestrator" {
  id = "RECONCILIATION-BATCH-ORCHESTRATOR-V1.0-20250705"
  scope = "PoC"
  cir = 98
  adi = 100
  sla_budget_minutes = 5

  consumes_from = topic.UnifiedDTOs_Input
  produces_to   = topic.Matching_Input_Topic

  uses = [
    common.dto,
    common.validation,
    common.config,
    common.security,
    common.resilience,
    common.error,
    common.monitoring
  ]

  overall_project_sla {
    duration_minutes = 30
    volume_combined_quants = 1000000
    nature = "EOD_one_shot_batch_job"
    start_point = "NaaS_Ingestion_Point"
  }

  organizational_context {
    project_structure = "Multi_module_Maven"
    poc_strategy = "CQ_Gemini_2_5_Pro_core_functionality_permissible_workarounds_actual_concurrency_parallelization_demonstrated"
    poc_data_volume_quants = 10000
  }

  component "kafka_consumer_buffer" {
    role = "Immediate entry point for Quants into Orchestrator. Temporary in-memory holding area."
    input_source_kafka_topic = topic.UnifiedDTOs_Input
    message_type = "Prepared batches of Quants (Unified DTOs). Each batch is a collection of transaction groups (e.g., {H1, W1}, {H2, W2}), where each group contains all Quants for a single, unique transaction identifier, co-located by the Orchestrator."
    upstream_producer = "NaaS"
    consumption_model = "Multiple Kafka Consumer Group instances run concurrently to pull DTOs from partitions"
    kafka_consumer_configuration {
      max_poll_records = 500
      fetch_min_bytes_kb = 512
      fetch_max_wait_ms = 100
      enable_auto_commit = false
      isolation_level = "read_committed"
    }
    number_of_consumer_instances = 2
    internal_buffer_capacity_quants = 1000
    overflow_behavior = "Apply internal backpressure to Kafka consumer (blocking polls) combined with explicit error logging if buffer persistently full for >5 seconds"
    processing_semantics = "At-least-once (default Kafka consumer)"
  }

  component "batch_preparation_arena" {
    role = "Where Unified DTOs are Grouped for Matching. Assembles DTOs into optimized batches."
    input_source = "Kafka Consumer Buffer"
    internal_parallelization {
      number_of_processing_units = 8
      work_distribution = "Shared ConcurrentHashMap for arena slots, fed by Kafka Consumer Buffer(s)"
    }
    logic_details {
      hash_based_grouping_key = "transactionId"
      batch_size_threshold_quants_per_batch = 100
      batch_timeout_ms = 5000
    }
  }

  component "matching_interface" {
    role = "Handoff to External Matching as a Service."
    input_source = "Batch Preparation Arena"
    parallelization_threads = 4
    mechanism = "Kafka Producer API to a high-throughput, asynchronous Kafka topic"
    kafka_producer_configuration {
      acks = "all"
      retries = 5
      batch_size_bytes = 16384
      linger_ms = 50
      delivery_timeout_ms = 60000
    }
    output_topic_kafka = topic.Matching_Input_Topic
  }

  component "results_ingestion_buffer" {
    role = "Temporary holding area for results returning from Matching as a Service."
    input_source_kafka_topic = topic.Matching_Output_Topic
    parallelization_consumers = 4
    capacity_quants = 1000
    overflow_behavior = "Apply internal backpressure to Kafka consumer (blocking polls)"
  }

  component "escalation_flow_buffer" {
    role = "Channels unmatched, anomalous, or problematic transactions for dedicated review and resolution via the Escalation flow."
    input_source = "Results Ingestion Buffer"
    capacity_quants = 100
    overflow_behavior = "Apply internal backpressure to Results Ingestion Buffer"
    output_topic_kafka = topic.Escalation_Topic
  }

  performance_and_resource_constraints {
    poc_throughput_target_quants_per_second = 5.5
    poc_throughput_average_over = "1_minute"
    latency_targets = [
      {
        description = "Kafka Ingestion to Matching_Input_Topic"
        value_seconds = 1
        percentile = "P99"
        operator = "<"
      },
      {
        description = "Matching_Output_Topic to Reporting_Input_Topic/Escalation_Topic"
        value_seconds = 1
        percentile = "P99"
        operator = "<"
      }
    ]
    resource_limits {
      max_ram_mb = 512
      max_cpu_usage_percent_of_single_core = 50
    }
  }

  cross_cutting_concerns {
    fault_tolerance_application {
      adaptive_pacing_kafka_producer = "Utilize central fault-tolerance library"
    }
  }
}

service "matcher" {
  id = "MATCHING-ENGINE-V1.0-GMNDS-20250704"
  scope = "PoC"
  cir = 98
  adi = 100
  sla_budget_minutes = 10

  consumes_from = topic.Matching_Input_Topic
  produces_to = [topic.Matching_Output_Topic, topic.Escalation_Topic, topic.Recon_Metadata_Topic]

  uses = [
    common.dto,
    common.validation,
    common.config,
    common.pci,
    common.security,
    common.resilience,
    common.error,
    common.monitoring
  ]

  overall_project_sla {
    duration_minutes = 30
    volume_combined_quants = 1000000
    nature = "EOD_one_shot_batch_job"
    start_point = "NaaS_Ingestion_Point"
  }

  organizational_context {
    project_structure = "Multi_module_Maven"
    poc_strategy = "CQ_Gemini_2_5_Pro_core_functionality_permissible_workarounds_actual_concurrency_parallelization_demonstrated"
    poc_data_volume_quants = 10000
  }

  component "kafka_consumer_buffer" {
    role = "Immediate entry point for Quants into Maas. Temporary in-memory holding area."
    input_source_kafka_topic = topic.Matching_Input_Topic
    message_type = "Prepared batches of Quants (Unified DTOs). Each batch is a collection of transaction groups (e.g., {H1, W1}, {H2, W2}), where each group contains all Quants for a single, unique transaction identifier, co-located by the Orchestrator."
    upstream_producer = "Reconciliation Batch Orchestrator (3. MATCHING INTERFACE)"
    consumption_model = "Multiple Kafka consumer group instances (concurrent workers) for high-throughput consumption"
    kafka_consumer_configuration {
      max_poll_records = 2000
      fetch_min_bytes_mb = 1
      fetch_max_wait_ms = 250
      enable_auto_commit = false
      isolation_level = "read_committed"
    }
    number_of_consumer_instances = 8
    internal_buffer_capacity_quants = 50000
    overflow_behavior = "Apply internal backpressure to Kafka consumer (blocking polls) combined with explicit error logging if buffer persistently full for >10 seconds"
    processing_semantics = "At-least-once (default Kafka consumer)"
  }

  component "matching_engine_core_cme" {
    role = "Central processing unit within Maas, performing multi-stage matching. Single logical component, internally partitioned by concurrent workers."
    input = "Pre-grouped transaction sets (e.g., {H1, W1}) from Maas's Kafka Consumer Buffer"
    internal_parallelization {
      cme_worker_pool_size_threads = 32
      concurrency_framework = "Utilize central fault-tolerance library for managing worker pools"
      work_distribution_to_workers = "Utilize a single shared ConcurrentLinkedQueue for worker consumption, fed by the Kafka Consumer Buffer"
    }
    waterfall_matching_algorithm = "Each CME worker processes a single transaction group through sequential steps. No internal hops between these stages per worker."
    stages {
      exact_match_module_soulmate {
        logic = "Compares Quants (Husband and Wife within the transaction group) on explicitly defined exact match keys"
        exact_match_keys = [
          { field = "transactionId", case_sensitive = true },
          { field = "amount", tolerance = 0.0 },
          { field = "currency", case_sensitive = true }
        ]
        uses_internal_lookups = true
        exit_condition = "If exact match found, processing for this transaction group ends, outputting Matched Quant."
      }
      fuzzy_match_module {
        execution_condition = "Executed ONLY if 1:1 Exact Match Fails."
        algorithm = "Levenshtein Distance is the base algorithm for string comparisons"
        optimization_for_scale = "Implement pre-filtering of candidates using n-gram indexing before applying Levenshtein Distance"
        configurable_tolerance = [
          {
            field = "description"
            algorithm = "Levenshtein_distance"
            value = 2
            operator = "<="
          },
          {
            field = "amount"
            algorithm = "amount_variance"
            value = 0.05
            unit = "USD"
            operator = "<="
          },
          {
            field = "amount"
            algorithm = "percentage_variance"
            value = 0.01
            unit = "percent"
            operator = "<="
          }
        ]
        uses_internal_lookups = true
        exit_condition = "If fuzzy match found, processing for this transaction group ends, outputting Fuzzy Matched Quant."
      }
      unmatched_exception_router_module {
        execution_condition = "Executed ONLY if both Exact and Fuzzy Match Fail."
        logic = "Tags Quants as 'Unreconciled & Exceptions' (URE)."
        reason_codes = [
          "NO_EXACT_MATCH",
          "FUZZY_TOLERANCE_EXCEEDED",
          "TIMING_DIFFERENCE_FLAG",
          "MALFORMED_DATA",
          "MISSING_COUNTERPARTY",
          "RULE_ENGINE_FAILURE",
          "OTHER_ISSUE"
        ]
        enrichment = "Adds detailed metadata explaining the failure, referencing relevant fields."
        output_to_dispatcher = "Handoff to Output Dispatcher for URE routing."
      }
    }
    output = "Matched, Fuzzy Matched, or URE Quants (tagged with match type/status, confidence score for fuzzy, and error details) are passed to the Output Dispatcher."
  }

  component "matching_rule_management" {
    role = "Manages, stores, and provides matching rules to CME."
    rule_definition_format = "YAML-based Domain-Specific Language (DSL) for matching rules"
    rule_storage = "Stored in Git repository, pulled at service startup"
    rule_updates = "Requires service restart for rule updates (accepted for POC). Future consideration: polling from Git every 5 minutes."
  }

  component "internal_state_lookups" {
    role = "Manages in-memory data required for CME (e.g., matching rules, reference data, potential pending Quants)."
    matching_rule_cache {
      purpose = "Store active matching rules for CME to avoid external lookups per transaction."
      sizing_mb = 50
      population_strategy = "Loaded at startup from Config Server (via common_config_utils)."
      refresh_invalidation_strategy = "Manual refresh via Actuator endpoint /actuator/refreshRules. Max staleness tolerance: 10 minutes."
      redundancy_ha = "Not required for POC (single instance)."
    }
    reference_data_cache {
      purpose = "Store master data or reference tables needed for matching/enrichment (e.g., list of valid merchant IDs, standard account types) in memory."
      sizing_mb = 200
      population_strategy = "Loaded at startup from internal configuration or a daily batch file."
      refresh_invalidation_strategy = "Scheduled refresh every 1 hour. Max staleness tolerance: 1 hour."
    }
    pending_quants_matching_window_state {
      purpose = "No cross-batch/temporal matching state required within Maas; Orchestrator guarantees co-located Quants per transaction ID within a single batch."
      storage_technology = "Not applicable"
      persistence_for_recovery = "Not applicable"
    }
  }

  component "reconciliation_metadata_management" {
    role = "Collects and manages per-batch/job reconciliation metadata and statistics within Maas for subsequent reporting."
    internal_buffer_job_meta {
      description = "In-memory, non-persistent buffer for collecting batchwise metadata about the recon process."
      structure = "ConcurrentHashMap<BatchId, BatchMetadataDTO> for aggregation within a batch."
      capacity_retention_last_batches = 100
      overflow_behavior = "LRU eviction"
    }
    recon_meta_dto_schema = "JSON Schema (defined in common_dto library)."
    data_points_collected = [
      "total Quants consumed",
      "total Quants produced",
      "matchedExactCount",
      "fuzzyMatchedCount",
      "ureCount",
      "processingTimeMs (for CME per batch)",
      "errorCounts (by type)",
      "resourceUtilizationSnapshot (CPU%, Mem% usage of Maas during batch processing)",
      "KafkaProducerMetrics (records_sent, produce_latency for this batch)"
    ]
    collection_logic = "CME workers will atomically update JOB_META as they process transaction groups within a batch."
    publishing_trigger = "After a full Orchestrator-defined batch is completely processed and its Quants are sent to output topics, the aggregated JOB_META for that batch is sent to the Output Dispatcher."
  }

  component "output_dispatcher" {
    role = "Handles routing of processed Quants from CME and JOB_META to respective Kafka topics."
    output_buffers_in_memory {
      recon_bucket {
        purpose = "In-memory buffer for Matched and Fuzzy Matched Quants."
        persistence = "NOT persistent."
        capacity_quants = 50000
        overflow_behavior = "Apply internal backpressure to CME (blocking output)."
      }
      ure_bucket {
        purpose = "In-memory buffer for Unreconciled & Exceptions (URE) Quants."
        persistence = "NOT persistent."
        capacity_quants = 5000
        overflow_behavior = "Apply internal backpressure to CME (blocking output)."
      }
      job_meta_buffer {
        purpose = "In-memory buffer for Recon-meta DTOs (from Recon-meta Management)."
        persistence = "NOT persistent."
        capacity_batch_metadata_dtos = 100
        overflow_behavior = "Oldest entries overwritten (LRU)."
      }
    }
    kafka_producers_internal {
      matching_output_producer {
        topic = topic.Matching_Output_Topic
        source_buffer = "RECON BUCKET"
        producer_configuration = "acks=all, retries=5, batch.size=16384 bytes, linger.ms=100ms. Utilize central fault-tolerance library for adaptive pacing."
      }
      escalation_output_producer {
        topic = topic.Escalation_Topic
        source_buffer = "URE BUCKET"
        producer_configuration = "acks=all, retries=5, batch.size=16384 bytes, linger.ms=100ms. Adaptive pacing highly recommended."
      }
      recon_meta_output_producer {
        topic = topic.Recon_Metadata_Topic
        source_buffer = "JOB_META Buffer"
        producer_configuration = "acks=all, retries=5, batch.size=16384 bytes, linger.ms=100ms. Adaptive pacing recommended."
      }
    }
    serialization_formats {
      matched_quants = "Avro with Schema Registry integration."
      ure_quants = "Avro with Schema Registry integration."
      recon_meta_dto = "Avro with Schema Registry integration."
    }
  }

  component "error_handling_dlq" {
    role = "Cross-cutting concern ensuring resilience and data integrity within Maas. Captures and isolates Quants failing internal Maas processing."
    error_categories = [
      "MATCH_RULE_EXECUTION_FAILURE",
      "INVALID_STATE_FOR_MATCH",
      "UNEXPECTED_DATA_PATTERN",
      "EXTERNAL_LOOKUP_FAILURE_TRANSIENT",
      "EXTERNAL_LOOKUP_FAILURE_PERMANENT",
      "BUFFER_OVERFLOW",
      "KAFKA_PRODUCER_FAILURE",
      "DESERIALIZATION_ERROR"
    ]
    dlq_strategy = "Dedicated Kafka DLQ topic (Maas_DLQ_Topic) with 7-day retention. Manual re-processing via external tool for failed messages."
    error_tagging = "Failed Quants (if routed to DLQ) are enriched with context (error code, message, timestamp, problematic fields, source transaction ID)."
    logging_strategy = "Detailed error logging via logging-monitoring central library, including original message snippets for debugging."
  }

  component "performance_scaling_manager" {
    role = "Cross-cutting concern ensuring Maas meets its SLA contributions and operates efficiently."
    metrics_collection {
      description = "Emits detailed metrics to Monitoring as a Service via logging-monitoring central library."
      metrics = [
        { name = "total Quants consumed", type = "counter" },
        { name = "total Quants produced", type = "counter" },
        { name = "matchedExactCount", type = "counter_per_second_per_batch" },
        { name = "fuzzyMatchedCount", type = "counter_per_second_per_batch" },
        { name = "ureCount", type = "counter_per_second_per_batch" },
        { name = "average_latency_per_match", type = "gauge", unit = "ms" },
        { name = "P99_latency_per_match", type = "gauge", unit = "ms" },
        { name = "internal_buffer_depths", type = "gauge" },
        { name = "CPU_Memory_Network_Disk_IO_utilization", type = "gauge_aggregated" },
        { name = "Kafka_consumer_lag", type = "gauge_per_partition", unit = "seconds" },
        { name = "Kafka_producer_rates", type = "gauge_per_topic", unit = "records_per_second" },
        { name = "cache_hit_ratio_lookups", type = "gauge" },
        { name = "cache_miss_ratio_lookups", type = "gauge" }
      ]
    }
    alerting {
      alert_rules = [
        {
          name = "Critical_Kafka_Consumer_Lag"
          severity = "CRITICAL"
          condition = "Kafka consumer lag > 60 seconds"
          notification_channel = "MonaaS, Central_Logging"
        },
        {
          name = "Critical_Maas_Latency_Spike"
          severity = "CRITICAL"
          condition = "P99 Maas latency > 500ms"
          notification_channel = "MonaaS, Central_Logging"
        },
        {
          name = "Critical_London_Bridge_URE_Count"
          severity = "CRITICAL"
          condition = "URE count > 100 for a batch"
          notification_channel = "MonaaS, Central_Logging"
        },
        {
          name = "Warning_Error_Rate_Exceeded"
          severity = "WARNING"
          condition = "Error rate > 0.1%"
          notification_channel = "MonaaS, Central_Logging"
        },
        {
          name = "Error_Buffer_Full"
          severity = "ERROR"
          condition = "Internal buffer is full"
          notification_channel = "MonaaS, Central_Logging"
        }
      ]
    }
    internal_scaling {
      description = "Dynamically adjusts CME worker pool size based on internal load (e.g., consumer buffer depth, CPU utilization)."
      logic_algorithm = "Dynamic adjustment of CME worker pool size (N) based on Kafka consumer buffer depth (if >75% capacity, N++ up to max; if <25% capacity, N-- down to min). Max N = 2x CPU cores, Min N = 4 threads."
    }
    external_scaling_hpa {
      description = "How Maas instances scale horizontally in Kubernetes/cloud environment."
      criteria = [
        {
          name = "Kafka_consumer_lag"
          source = "KEDA"
          target_value = 30
          unit = "seconds"
        },
        { name = "average_CPU_utilization", value = 60, unit = "percent" }
      ]
      replicas_min = 2
      replicas_max = 8
    }
    distributed_tracing = "Use of distributed tracing (e.g., OpenTelemetry via security central library) to trace a single transaction's journey through Maas stages."
  }

  cross_cutting_concerns {
    fault_tolerance_application {
      adaptive_pacing_kafka_producer = "Utilize central fault-tolerance library"
    }
    testing_strategy {
      unit_tests = "High coverage for matching algorithms and rule logic."
      integration_tests = "Kafka consumer/producer interactions, rule loading, internal component interactions."
      performance_tests = "Simulated transaction matching up to 10,000 Quants/day to validate SLA slice and latency targets."
      contract_tests = "Schema compatibility with Matching_Input_Topic and all output topics (Matching_Output_Topic, Escalation_Topic, Recon_Metadata_Topic)."
    }
  }
}

service "reporter" {
  id = "REPAAS-POC-V1.0-20250704"
  scope = "PoC"
  cir = 95
  adi = 100
  aliases = "RepaaS"
  semantic_role = "analytical_reporting_from_kafka_persistence"
  classification = "support_service"
  criticality_level = "tier_2_post_reconciliation"

  consumes_from = [topic.Recon_Metadata_Topic, topic.Escalation_Topic, topic.Matching_Output_Topic]

  uses = [
    common.dto,
    common.config,
    common.security,
    common.error,
    common.monitoring
  ]

  processing_semantic {
    reporting_scope_and_granularity {
      job_level_reconciliation_summary {
        description = "Single report for an entire 30-minute job, printing all batches chronologically within that job."
        content_detail = "Batch ID, total input quants, matched count, URE count per batch, arranged chronologically within a job report."
        granularity = "Job-level aggregation of batch data"
      }
      weekly_ure_trend_report {
        description = "Counts of UREs by reason code over time."
        granularity = "Daily/Weekly Aggregation"
      }
    }
    data_sources_and_model {
      persistence_layer {
        database_type = "Kafka_Streams_State_Store"
        role = "Internal, localized, fault-tolerant persistence for read models."
        technology = "Apache Kafka Streams (KTable/KStream aggregations)"
        note = "Strictly no external database connection (e.g., MongoDB)."
      }
      internal_data_model_conceptual = "Kafka Streams KTables for materialized views of job summaries, batch details, and URE trends."
    }
    query_patterns_and_performance {
      expected_query_patterns = "Querying Kafka Streams local state stores by job ID, date range, URE reason codes."
      query_response_times {
        job_level_report_generation_p95_seconds = 5
        weekly_trend_reports_p95_seconds = 10
      }
    }
    data_latency_and_freshness {
      report_availability_latency_minutes = 15
      max_data_staleness_minutes = 15
      consistency_model = "Eventual Consistency"
    }
    user_concurrency_and_report_generation_volume {
      concurrent_users_viewing_reports = 5
      report_generation_volume {
        daily_job_reports_generated = 1
        weekly_trend_reports = 5
        api_exports_per_day = 10
      }
      poc_data_volume_quants_max_per_job = 10000
    }
    report_distribution_and_delivery {
      delivery_mechanism = ["Web UI Access", "Report Export API"]
      web_ui_endpoint = "/reports"
      export_api_endpoints = [
        {
          type = "Job_Report_Export"
          uri = "/api/reports/job/{jobId}/export"
          http_method = "GET"
          export_formats = ["CSV", "JSON"]
        },
        {
          type = "URE_Trend_Report_Export"
          uri = "/api/reports/ure/trend/export"
          http_method = "GET"
          export_formats = ["CSV", "JSON"]
        }
      ]
      security_model = "Basic authentication for POC scope (API and UI)."
    }
    scalability_and_archival {
      expected_growth_period_months = 12
      projected_growth_target {
        data_volume_increase_x = 2
        concurrent_users_increase_x = 2
        report_generation_increase_x = 2
      }
      future_solution_recommendation = "Scaling Kafka Streams instances, dedicated reporting microservice cluster, integration with external BI tools/data warehouses (potentially with more robust external persistence)."
      archival_strategy = "Offload historical Kafka topic data older than 1 year to cold storage (e.g., S3); Kafka Streams state stores would rebuild from compacted topics upon restart/rebalance."
    }
  }

  resource_constraints_for_poc {
    max_ram_mb = 1024
    max_cpu_usage_percent_of_single_core = 75
  }

  sub_components_semantic {
    kafka_consumer_group {
      order = 1
      responsibility = "Parallel consumption of Kafka input topics (Recon_Metadata, Escalation, Matching_Output)"
    }
    data_buffer {
      order = 2
      responsibility = "In-memory buffer for incoming Kafka messages before stream processing"
    }
    report_preparation_arena {
      order = 3
      responsibility = "Stateful Kafka Streams application for aggregating/materializing data for job-level and trend reports (KTables)"
    }
    report_generator {
      order = 4
      responsibility = "Transforms materialized report data from Kafka Streams state stores into final report format (CSV/JSON)"
    }
    export_rest_api {
      order = 5
      responsibility = "Exposes HTTP endpoints for report generation requests and downloading generated reports"
    }
  }

  cross_cutting_concerns {
    fault_tolerance_application {
      kafka_streams_resilience = "Leverages Kafka Streams' fault tolerance for state store recovery and processing guarantees."
    }
    testing_strategy {
      unit_tests = "High coverage for Kafka Streams topologies, report generation logic, and API endpoints."
      integration_tests = "Kafka Streams application end-to-end with simulated Kafka topics, API endpoint testing."
      performance_tests = "Simulated report generation and API export load to validate query response times and resource constraints."
      contract_tests = "Schema compatibility with input Kafka topics and output report formats (CSV/JSON)."
    }
  }
}

service "escalator" {
  id = "ESCAL-SVC-001-PAIRCODE-V1.0-20250705"
  scope = "PoC"
  cir = 95
  adi = 100
  aliases = "ExaaS"
  semantic_role = "human_workflow_ure_resolution"
  classification = "business_support_service"
  criticality_level = "tier_2_post_reconciliation"

  consumes_from = topic.Escalation_Topic
  persistence = database.ure_mongo_db

  uses = [
    common.dto,
    common.validation,
    common.config,
    common.pci,
    common.security,
    common.resilience,
    common.error,
    common.monitoring
  ]

  processing_semantic {
    objective {
      description = "Develop a functional Proof of Concept (POC) for Escalation-as-a-Service (ExaaS)."
      quantified_goal = "Deliver a working POC of ExaaS within 90 minutes of dedicated pair-coding time."
      metrics = [
        "Code Completeness: Core ExaaS components functional.",
        "Functional Flow: Demonstration of URE ingestion, persistence, status update, and API interaction.",
        "Alert Trigger: Ability to demonstrate 'London Bridge' alert on exceeding 100 UREs for a job."
      ]
    }
    in_scope_functionality = [
      {
        function = "Consume UREs from Kafka"
        source = topic.Escalation_Topic
        mechanism = "Kafka Consumer Group (KCG)"
        constraints = {
          max_consumption_rate_ures_per_second_peak = 5
          max_consumer_lag_tolerance_seconds = 10
        }
      },
      {
        function = "Manage internal buffers for data flow"
        mechanism = "java.util.concurrent.LinkedBlockingQueue"
        detail = "From KCG to MongoDB persistence system."
        constraints = {
          buffer_capacity_ures = 5000
          batch_write_size_ures_per_batch = 100
          overflow_behavior = "Route to Kafka DLQ topic"
          dlq_kafka_topic_name = topic.ure_dlq_topic
        }
      },
      {
        function = "Read/write UREs to MongoDB"
        role = "Source of truth ledger for unmatched/weird items."
        constraints = {
          max_rw_qps_peak = 10
          target_persistence_latency_p99_ms = 100
        }
      },
      {
        function = "Support 'Inspection Arena' concept"
        detail = "UREs presented for manual inspection by `bankops` (no UI from ExaaS)."
      },
      {
        function = "Implement 'Status Tracker'"
        detail = "Observe UREs entering Inspection Arena and update their status in MongoDB Ledger."
      },
      {
        function = "Implement 'Lean but Powerful Curing Algorithm'"
        detail = "Human-dependent workflow to try and correct URE issues inside Inspection Arena (not automated curing)."
        constraints = {
          workflow_states = ["PENDING_REVIEW", "IN_PROGRESS", "RESOLVED", "REJECTED", "ESCALATED"]
        }
      },
      {
        function = "Provide 'Escalation Curing API'"
        type = "REST API"
        consumer = "`bankops`"
        detail = "To interact with UREs in Inspection Area (cure, update status)."
        constraints = {
          api_endpoints = [
            {uri = "/api/ures/{id}/cure", method = "POST"},
            {uri = "/api/ures/{id}/status", method = "PUT"}
          ]
          request_response_schemas = [
            {name = "CureRequest", type = "JSON_SCHEMA_REFERENCE"},
            {name = "StatusUpdateRequest", type = "JSON_SCHEMA_REFERENCE"},
            {name = "UreResponse", type = "JSON_SCHEMA_REFERENCE"}
          ]
          custom_error_codes = ["URE_NOT_FOUND", "INVALID_STATUS_TRANSITION"]
          target_api_latency_p99_ms = 200
        }
      },
      {
        function = "Trigger 'SYSTEM ALERT: Code London Bridge Is Falling Down'"
        condition = "UREs exceed 100 (1% of 10K total load in a 30min job)."
        constraints = {
          alert_mechanism = "Log to system stdout/file, publish to monitoring Kafka topic (if integrated)"
          alert_notification_channel = "System logs, dedicated monitoring dashboard"
        }
      }
    ]
    out_of_scope_functionality = [
      "Automated URE curing beyond simple rules or workflow management.",
      "Full historical reporting/auditing within ExaaS beyond the MongoDB ledger's current state (delegated to external systems).",
      "Deployment to Docker/Cloud environments (POC is native Windows only).",
      "High Availability (HA) or automated failover for the single ExaaS instance (POC constraint).",
      "Graphical User Interface (GUI) for Inspection Arena or Curing API (API only)."
    ]
  }

  technology_alignment {
    programming_language = "Java 21"
    framework = "Spring Boot"
    build_tool = "Maven"
    message_broker = "Apache Kafka"
    stream_processing = "Apache Kafka Streams"
    persistence = "MongoDB (for URE Ledger)"
    api_framework = "Spring Web"
    common_libraries = [
      "common_dto",
      "validation_utils",
      "config_utils",
      "pci_dss",
      "security",
      "fault_tolerance",
      "error_handling",
      "logging_monitoring"
    ]
  }

  risk_assessment_details = [
    {
      id = "ESCAL-R-001"
      type = "Performance/Resource"
      status = "Mitigated by Design"
      description = "URE Volume: System design optimized for high volume. POC load is 50K txns total; 'London Bridge' alert covers high URE count (100)."
      mitigation = "Internal buffers, batch writing, Kafka Consumer Group for parallel consumption, robust MongoDB configuration for scale."
      quantified_context = {
        max_ure_throughput_poc_ures_per_second_peak = 5
        cpu_memory_target_peak_load = {
          max_ram_mb = 512
          max_cpu_usage_percent_single_core = 50
        }
      }
    },
    {
      id = "ESCAL-R-002"
      type = "SinglePointOfFailure"
      status = "Accepted for POC"
      description = "The POC will run as a single instance, making it a SPOF."
      mitigation_for_prod = "Implement multi-instance deployment with Kafka Consumer Group for HA and load balancing."
    },
    {
      id = "ESCAL-R-003"
      type = "Consistency"
      status = "Mitigated by Design"
      description = "Status Consistency: Achieved using `read-your-writes` consistency via MongoDB session transactions or `majority` write concern with `primary` read preference on replica set."
      mitigation = "Appropriate MongoDB write/read concerns and transaction implementation."
    },
    {
      id = "ESCAL-R-004"
      type = "Audit/Compliance"
      status = "Mitigated by Design"
      description = "MongoDB Auditability/Transactions: Achieved using `MongoDB Multi-Document Transactions` for atomicity, ensuring `immutable fields` (`createdAt`, `createdBy`), and capturing `Change Streams` for an external audit log."
      mitigation = "Robust schema design, transaction usage, and external audit log integration."
    },
    {
      id = "ESCAL-R-005"
      type = "BufferManagement"
      status = "Mitigated by Design"
      description = "Buffer Specifics: Managed by `java.util.concurrent.LinkedBlockingQueue` for in-memory, with configured capacity, `batching writes` to MongoDB, and routing overflow to a Kafka `DLQ` topic."
      mitigation = "Explicit buffer design and error handling (DLQ)."
    },
    {
      id = "ESCAL-R-006"
      type = "ImplementationComplexity"
      status = "Mitigated by Design"
      description = "Curing Algorithm Workflow: Defined as a human-dependent workflow with clear steps and an alert threshold, reducing coding ambiguity."
      mitigation = "Focus on implementing key workflow states and API interactions; leverage Gemini Pro for rapid code generation for known patterns."
    },
    {
      id = "ESCAL-R-007"
      type = "APIContractDefinition"
      status = "Mitigated by Design"
      description = "API Contracts: Clear endpoints, request/response schemas (JSON), and explicit error codes (HTTP + custom) for `Escalation Curing API` defined."
      mitigation = "Automated code generation for API interfaces based on defined contracts."
    },
    {
      id = "ESCAL-R-008"
      type = "EnvironmentSpecific"
      status = "Mitigated by Override"
      description = "Windows Native Environment: Potential setup/compatibility issues."
      mitigation = "Resolved by `override_cq2025` grant; environment will be prepared/managed to ensure readiness."
    },
    {
      id = "ESCAL-R-009"
      type = "DependencyManagement"
      status = "Mitigated by Override"
      description = "Common Library Access/Configuration: Potential time loss on dependency issues."
      mitigation = "Resolved by `override_cq2025` grant; common libraries will be confirmed/configured."
    }
  ]

  cross_cutting_concerns {
    fault_tolerance_application {
      retries = "Configured for external calls (e.g., MongoDB, Kafka Producer)"
      circuit_breakers = "Applied to external dependencies (e.g., MongoDB, Kafka Producer)"
      timeouts = "Configured for all external I/O operations"
    }
    testing_strategy {
      unit_tests = "High coverage for API logic, MongoDB interactions, and URE status transitions."
      integration_tests = "Kafka consumer/producer interactions, MongoDB persistence, API endpoint testing."
      performance_tests = "Simulated API calls and URE ingestion load to validate QPS and latency targets."
      contract_tests = "Schema compatibility with Kafka input topics and API request/response formats."
    }
  }
}

service "monitor" {
  id = "MON-SVC-001-FINAL"
  scope = "PoC"
  cir = 98
  adi = 100
  aliases = "MonaaS"
  semantic_role = "realtime_metrics_aggregation_alerting"
  classification = "support_service"
  criticality_level = "tier_2_for_visibility_tier_1_for_reporting_core_sla"

  monitors = [service.naas, service.orchestrator, service.matcher, service.reporter, service.escalator]

  uses = [common.monitoring, common.error]

  processing_semantic {
    input_model {
      mechanism = "HTTP_Polling"
      source_microservices = ["NaaS", "RecBatchOrch", "Maas", "Reporting", "Escalation"]
      endpoints_polled = [
        { type = "standard_actuator_health", uri = "/actuator/health" },
        { type = "custom_actuator_job_status", uri = "/actuator/job-status" }
      ]
    }
    metrics_collected {
      standard_actuator_metrics = [
        { name = "jvm.memory.used", tags = "service_name" },
        { name = "jvm.memory.max", tags = "service_name" },
        { name = "jvm.threads.live", tags = "service_name" },
        { name = "system.cpu.usage", tags = "service_name" },
        { name = "process.uptime", tags = "service_name" },
        { name = "http.server.requests.count", tags = "service_name,uri,method,status" },
        { name = "http.server.requests.duration.p95", tags = "service_name,uri,method" }
      ]
      custom_job_specific_metrics = [
        {
          name = "reconciliation.job.status"
          tags = "service_name,job_id"
          type = "Enum(STARTED,IN_PROGRESS,COMPLETED,FAILED)"
        },
        {
          name = "reconciliation.job.start.time"
          tags = "service_name,job_id"
          type = "Timestamp"
        },
        {
          name = "reconciliation.job.end.time"
          tags = "service_name,job_id"
          type = "Timestamp"
        },
        {
          name = "reconciliation.transactions.processed.total"
          tags = "service_name,job_id"
          type = "Counter"
        }
      ]
      overall_job_metrics_calculated_by_monaas = [
        { name = "overall.reconciliation.job.elapsed.time" },
        { name = "overall.reconciliation.job.status" }
      ]
    }
    collection_frequency {
      polling_interval_seconds = 10
    }
    data_retention {
      current_state_store {
        type = "in_memory"
        retention_period_unit = "hour"
        retention_period_value = 1
        scope = "latest_granular_per_service"
      }
      job_run_history_log {
        type = "file_log"
        retention_period_unit = "hours"
        retention_period_value = 24
        scope = "overall_job_summary"
      }
    }
    alerting_logic {
      alert_rules = [
        {
          name = "Critical_Service_Down"
          severity = "CRITICAL"
          condition = "ServiceHealthStatus is DOWN for 2 consecutive polls (20 seconds)"
          notification_channel = "stdout, monaas-alerts.log"
        },
        {
          name = "Critical_Job_Failure"
          severity = "CRITICAL"
          condition = "overall.reconciliation.job.status is FAILED"
          notification_channel = "stdout, monaas-alerts.log"
        },
        {
          name = "Critical_SLA_Breach"
          severity = "CRITICAL"
          condition = "overall.reconciliation.job.elapsed.time > 30 minutes at job completion"
          notification_channel = "stdout, monaas-alerts.log"
        },
        {
          name = "Warning_High_CPU"
          severity = "WARNING"
          condition = "system.cpu.usage > 85% for 3 consecutive polls (30 seconds)"
          notification_channel = "stdout, monaas-alerts.log"
        },
        {
          name = "Warning_High_Error_Rate"
          severity = "WARNING"
          condition = "http.server.requests.count (for errors) indicates error rate > 1% of total requests over last 1 minute"
          notification_channel = "stdout, monaas-alerts.log"
        }
      ]
    }
    data_volume_projection {
      unique_time_series_estimate = 120
      data_points_per_second_ingestion = 12
      current_state_store_memory_estimate_mb = 50
    }
    latency_targets {
      metric_to_dashboard_display_latency_seconds = 15
      alert_condition_to_logged_latency_seconds = 5
    }
    future_scalability {
      current_capacity_microservices = 5
      expected_growth_period_months = 12
      projected_growth_target = "10_microservices (2x), 2x_metric_volume"
      future_solution_recommendation = "Dedicated Prometheus/Grafana stack with persistent time-series storage"
    }
  }

  sub_components_semantic {
    service_polling_agent {
      responsibility = "Initiates_HTTP_polling_requests"
    }
    raw_status_ingester {
      responsibility = "Parses_and_and_validates_raw_HTTP_responses"
    }
    overall_job_state_processor {
      responsibility = "Aggregates_and_calculates_overall_job_status"
    }
    current_state_store {
      responsibility = "In_memory_cache_for_latest_statuses"
    }
    dashboard_renderer {
      responsibility = "Generates_simple_web_UI"
    }
    sla_health_alerting {
      responsibility = "Monitors_state_triggers_log_alerts"
    }
  }

  cross_cutting_concerns {
    testing_strategy {
      unit_tests = "High coverage for polling logic, metric aggregation, and alert condition evaluation."
      integration_tests = "Simulated HTTP polling against mock service endpoints, alert logging verification."
      performance_tests = "Simulated metric ingestion volume to validate latency targets and resource utilization."
      contract_tests = "Verification of expected metric formats from polled endpoints."
    }
  }
}