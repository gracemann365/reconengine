âœ¦ The matcher module is a Spring Boot microservice designed to perform core reconciliation matching logic on financial transactions. It acts
  as a critical component in the overall reconciliation engine, taking in transaction data, applying a set of predefined matching rules, and
  then outputting the results to various Kafka topics for further processing by other services.

  Here's a detailed technical breakdown of its working:


   1. Project Structure and Core Dependencies (`pom.xml`, `MatcherApplication.java`):
       * The matcher module is a standard Maven project, inheriting from a parent reconengine POM.
       * It's built as a Spring Boot application, indicated by spring-boot-starter-web and spring-boot-starter-actuator dependencies,
         providing web capabilities and production-ready features like monitoring.
       * spring-kafka is a key dependency, enabling seamless integration with Apache Kafka for both consuming input and producing output.
       * It depends on the common module (a local project dependency), which likely contains shared DTOs (Data Transfer Objects) like Quant
         and QuantPair, and potentially common utilities or configurations.
       * spring-boot-starter-security is included for basic security, primarily for its Actuator endpoints.
       * MatcherApplication.java serves as the main entry point, bootstrapping the Spring Boot application. The @ComponentScan annotation
         ensures that components from both com.grace.recon.matcher and com.grace.recon.common packages are discovered and managed by Spring.


   2. Kafka Configuration (`KafkaConfig.java`, `application.yml`):
       * `application.yml`: This YAML file defines the Kafka broker connection (bootstrap-servers) and various producer properties
         (acknowledgments, retries, serializers for key and value). Crucially, it specifies the Kafka topics used by the matcher service:
         matching-input (for incoming data), matching-output, escalation, recon-metadata (for outgoing results), and matcher-dlq (for
         dead-letter queue).
       * `KafkaConfig.java`: This class centralizes the Kafka configuration for the matcher service.
           * It defines a DefaultErrorHandler bean, which is crucial for handling messages that fail processing. This error handler uses a
             DeadLetterPublishingRecoverer to automatically forward failed messages to the matcher-dlq topic. This ensures that messages
             that cause processing errors are not lost but are instead quarantined for later inspection and reprocessing.
           * It also defines a ConcurrentKafkaListenerContainerFactory bean, which is used by Kafka listeners to create consumer instances.
             This factory is configured to use the DefaultErrorHandler for common error handling across all Kafka listeners in the
             application.
           * It explicitly states that ProducerFactory and KafkaTemplate beans are auto-configured by Spring Boot based on application.yml,
             avoiding manual and potentially conflicting bean definitions.


   3. Input Processing - Kafka Consumer (`MatchingInputConsumer.java`, `QuantBufferService.java`):
       * `MatchingInputConsumer`: This component is responsible for consuming QuantPair objects from the matching-input Kafka topic.
           * It uses the @KafkaListener annotation to define its topic, consumer group ID, and concurrency. The containerFactory is set to
             kafkaListenerContainerFactory (from KafkaConfig), ensuring proper error handling.
           * It consumes messages in batches (List<QuantPair> batch) for efficiency.
           * Upon receiving a batch, it iterates through each QuantPair and adds it to an internal buffer managed by QuantBufferService.
           * After successfully adding all pairs to the buffer, it acknowledges the Kafka offsets (acknowledgment.acknowledge()), signaling
             that these messages have been successfully processed and can be committed.
           * Error logging is in place to capture issues during batch processing without committing offsets, allowing for potential
             re-delivery.
           * It integrates with Micrometer to increment a messagesConsumedCounter metric, providing observability into the input throughput.
       * `QuantBufferService`: This service acts as an in-memory buffer (a LinkedBlockingQueue) for incoming QuantPair objects.
           * It has a defined BUFFER_CAPACITY (25,000 QuantPairs, representing 50,000 quants), which is a critical backpressure mechanism.
           * The addToBuffer() method uses buffer.put(quantPair), which is a blocking operation. If the buffer is full, the Kafka consumer
             thread will pause, effectively slowing down consumption from Kafka until space becomes available. This prevents the matcher
             service from being overwhelmed by a high ingress rate.
           * The takeFromBuffer() method is used by the matching engine to retrieve QuantPairs for processing, also blocking if the buffer
             is empty.
           * It exposes a Micrometer Gauge to track the current buffer size, providing real-time visibility into the queue depth.


   4. Core Matching Logic (`CoreMatchingEngine.java`, `RuleService.java`, `MatchingRules.java`, `MatchingUtil.java`,
      `ReferenceDataService.java`):
       * `CoreMatchingEngine`: This is the central component where the actual reconciliation matching happens.
           * It's initialized with QuantBufferService (for input), RuleService (for rules), ResultPublisher (for output), and MeterRegistry
             (for metrics).
           * Lifecycle Management: @PostConstruct startEngine() initiates a dedicated processingLoop in a fixed thread pool (workerPool).
             @PreDestroy stopEngine() gracefully shuts down the thread pool.
           * `processingLoop()`: Continuously polls QuantPairs from the QuantBufferService using bufferService.takeFromBuffer() and submits
             them to the workerPool for parallel processing by processTransaction().
           * `processTransaction(QuantPair pair)` (Waterfall Matching): This method implements the core matching algorithm:
               * Exact Match: It first attempts an exact match using isExactMatch(). If successful, it publishes a MatchResult of type EXACT
                 and increments exactMatchCounter.
               * Fuzzy Match: If no exact match is found, it proceeds to isFuzzyMatch(). If successful, it publishes a MatchResult of type
                 FUZZY and increments fuzzyMatchCounter.
               * Unmatched Exception (URE) Router: If neither exact nor fuzzy matches are found, the QuantPair is considered an unmatched
                 exception. It then creates and publishes a UreQuant object and increments ureCounter.
           * `isExactMatch()` and `isFuzzyMatch()`: These methods dynamically apply matching rules defined in MatchingRules. They use
             reflection (getFieldValue()) to access fields of Quant objects based on rule configurations.
           * `getFieldValue()`: A utility method that uses Java Reflection to get the value of a specified field from a Quant object. It
             employs a fieldCache (a ConcurrentHashMap) to store Field objects, optimizing performance by avoiding repeated reflection
             lookups.
           * `checkTolerance()`: Used by fuzzy matching to apply different algorithms (e.g., Levenshtein distance for strings, absolute
             variance for numbers) based on the rule's configuration.
       * `RuleService`: Manages the loading and validation of matching rules.
           * It reads rules from rules.yml (a YAML file) using ObjectMapper (Jackson library).
           * @PostConstruct loadAndValidateRules() ensures rules are loaded and validated at application startup. A critical failure to load
             or validate rules will cause the application to shut down, preventing it from running with incorrect logic.
           * @WriteOperation refreshRules() exposes an Actuator endpoint (/actuator/rules) that allows rules to be reloaded dynamically at
             runtime without restarting the application. This is a powerful feature for agile rule management.
       * `MatchingRules.java`: A POJO that defines the structure of the matching rules, including ExactMatch keys and FuzzyMatch tolerances
         (field, algorithm, value). It includes a validateFields() method to ensure that the fields specified in the rules actually exist in
         the Quant DTO, preventing runtime errors due to misconfigurations.
       * `MatchingUtil.java`: A utility class providing static methods for common matching algorithms, such as calculateLevenshteinDistance
         (for string similarity) and calculateAbsoluteDifference/calculatePercentageVariance (for numerical comparisons).
       * `ReferenceDataService`: Loads and caches static reference data (e.g., merchant IDs to names). This data can be used during the
         matching process for lookups or enrichment. It also has a placeholder for future refresh mechanisms.


   5. Output Processing - Kafka Producer (`ResultPublisher.java`):
       * `ResultPublisher`: This service is responsible for publishing the results of the matching process to various Kafka topics.
           * It uses a KafkaTemplate<String, Object> to send messages. The Object type for the value allows it to send different DTOs
             (MatchResult, UreQuant, ReconMeta) as JSON, leveraging Spring Kafka's JsonSerializer (configured in application.yml).
           * It has dedicated methods for publishing MatchResult (to matching-output), UreQuant (to escalation), and ReconMeta (to
             recon-metadata).
           * The sendMessage() method sends messages asynchronously using CompletableFuture, meaning the calling thread is not blocked. It
             includes whenComplete callbacks to log success or failure, ensuring that publishing outcomes are monitored.


   6. Security (`SecurityConfig.java`):
       * SecurityConfig configures basic HTTP security for the application.
       * It specifically secures the /actuator/rules endpoint, requiring an ADMIN role for access. This is critical for preventing
         unauthorized modification of matching rules.
       * Other /actuator/** endpoints are permitted for general monitoring purposes.
       * All other requests require authentication.


   7. Testing (`MatcherKafkaIntegrationTest.java`, `CoreMatchingEngineTest.java`, `application-test.yml`):
       * `MatcherKafkaIntegrationTest`: An integration test that uses EmbeddedKafkaBroker to simulate a Kafka environment.
           * It autowires the main application's KafkaTemplate to send test messages.
           * It programmatically creates a test-specific KafkaConsumer for the DLQ topic (matcher-dlq) to verify that "poison pill" messages
             (invalid JSON) are correctly routed to the DLQ by the DefaultErrorHandler. This ensures the robustness of the error handling
             mechanism.
       * `CoreMatchingEngineTest`: A unit test for the CoreMatchingEngine using Mockito. It focuses on testing the core matching logic in
         isolation, mocking dependencies like QuantBufferService, RuleService, and ResultPublisher. It includes a test to ensure the engine
         throws an IllegalStateException if rules are not properly loaded.
       * `application-test.yml`: Provides test-specific Kafka consumer properties, such as the group-id and bootstrap-servers, overriding
         the main application's configuration for isolated testing.


  In summary, the matcher module is a robust, event-driven microservice that consumes transaction data from Kafka, applies sophisticated
  matching logic based on configurable rules, and publishes the categorized results back to Kafka. It incorporates best practices for
  resilience (DLQ, backpressure), observability (metrics, logging), and security (Actuator endpoint protection), making it a critical and
  well-engineered component of the reconciliation engine.
